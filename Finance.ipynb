{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaeb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import findspark\n",
    "\n",
    "import findspark\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "671e8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime and pandas function\n",
    "\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b868f363",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='logfile.txt', level=logging.INFO, format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f3f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch findspark.init()\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57f68db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this to launch a spark session\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d935617",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"All packages are imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6915cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Finance\").master(\"local\").enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c76b4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Spark Session created {}\".format(spark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4669f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DoubleType, LongType, TimestampType, DateType\n",
    "\n",
    "# Create an empty RDD\n",
    "data = spark.sparkContext.emptyRDD()\n",
    "\n",
    "fileSchema = StructType([StructField('ISIN', StringType(),True),\n",
    "                        StructField('Mnemonic', StringType(),True),\n",
    "                        StructField('SecurityDesc', StringType(),True),\n",
    "                        StructField('SecurityType', StringType(),True),\n",
    "                        StructField('Currency', StringType(),True),\n",
    "                        StructField('SecurityID', StringType(),True),\n",
    "                        StructField('Date', DateType(),True),\n",
    "                        StructField('Time', TimestampType(),True),\n",
    "                        StructField('StartPrice', StringType(),True),\n",
    "                        StructField('MaxPrice', StringType(),True),\n",
    "                        StructField('MinPrice', StringType(),True),\n",
    "                        StructField('EndPrice', StringType(),True),\n",
    "                        StructField('TradedVolume', StringType(),True),\n",
    "                        StructField('NumberOfTrades', StringType(),True),])\n",
    " \n",
    "\n",
    "# Create an empty RDD with empty schema\n",
    "df_initial = spark.createDataFrame(data = data,\n",
    "                             schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddd400e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Intial schema with a blank database created. Columns are {}\".format(df_initial.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "923e12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing date\n",
    "test_date = datetime.datetime.strptime(\"03-01-2022\", \"%d-%m-%Y\")\n",
    "date_generated = pd.date_range(test_date, periods=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a96834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2022-01-03', '2022-01-04'], dtype='datetime64[ns]', freq='D')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "538b3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Date of files which we will be reading {}\".format(date_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c4b9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data into the dataframe from files\n",
    "\n",
    "for jdx in date_generated:    \n",
    "    if jdx.weekday() >= 0 and jdx.weekday() <= 4:\n",
    "        for idx in range(0, 24):\n",
    "            if idx > 9:\n",
    "                fname = \"dataset/\" + str(jdx.date()) + \"/\" + str(jdx.date()) + \"_BINS_XETR\" + str(idx) + \".csv\"\n",
    "            else:\n",
    "                fname = \"dataset/\" + str(jdx.date()) + \"/\" + str(jdx.date()) + \"_BINS_XETR0\" + str(idx) + \".csv\"\n",
    "            \n",
    "            logging.info(\"File name {} is picked to be loaded\".format(fname))\n",
    "            df = spark.read.load(fname, format=\"csv\", schema = fileSchema, header = True)\n",
    "            logging.info(\"File {} is loaded now. It has {} records and \".format(fname, df.count()))\n",
    "            df_initial = df_initial.union(df)\n",
    "            logging.info(\"Records are merged into main records. Total records now are: {}\".format(fname, df.count()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95dbe2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ISIN: string (nullable = true)\n",
      " |-- Mnemonic: string (nullable = true)\n",
      " |-- SecurityDesc: string (nullable = true)\n",
      " |-- SecurityType: string (nullable = true)\n",
      " |-- Currency: string (nullable = true)\n",
      " |-- SecurityID: string (nullable = true)\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- StartPrice: string (nullable = true)\n",
      " |-- MaxPrice: string (nullable = true)\n",
      " |-- MinPrice: string (nullable = true)\n",
      " |-- EndPrice: string (nullable = true)\n",
      " |-- TradedVolume: string (nullable = true)\n",
      " |-- NumberOfTrades: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column names in the dataframe\n",
    "\n",
    "df_initial.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ee42778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+----------+-------------------+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|      Date|               Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-------------------+----------+--------+--------+--------+------------+--------------+\n",
      "|AT0000A0E9W5|    SANT|         S+T AG O.N.|Common stock|     EUR|   2504159|2022-01-03|2022-10-28 08:00:00|     14.76|   14.76|   14.75|   14.75|        4414|             2|\n",
      "|DE000A0DJ6J9|     S92|SMA SOLAR TECHNOL.AG|Common stock|     EUR|   2504287|2022-01-03|2022-10-28 08:00:00|     37.64|   37.66|    37.6|   37.66|        1649|             3|\n",
      "|DE000A0D6554|    NDX1|      NORDEX SE O.N.|Common stock|     EUR|   2504290|2022-01-03|2022-10-28 08:00:00|     13.99|   14.03|   13.94|   13.96|       23011|            36|\n",
      "|DE000A0D9PT0|     MTX|MTU AERO ENGINES ...|Common stock|     EUR|   2504297|2022-01-03|2022-10-28 08:00:00|       180|  180.05|   179.5|   179.5|        2308|            22|\n",
      "|DE000A0HN5C6|    DWNI|DEUTSCHE WOHNEN S...|Common stock|     EUR|   2504314|2022-01-03|2022-10-28 08:00:00|     37.28|   37.28|   37.28|   37.28|        2897|             1|\n",
      "|DE000A0JL9W6|     VBK|VERBIO VER.BIOENE...|Common stock|     EUR|   2504343|2022-01-03|2022-10-28 08:00:00|        60|      60|   59.35|      60|        8883|            30|\n",
      "|DE000A0LD2U1|     AOX|ALSTRIA OFFICE RE...|Common stock|     EUR|   2504379|2022-01-03|2022-10-28 08:00:00|     19.53|   19.53|   19.53|   19.53|       16703|            13|\n",
      "|DE000A0LD6E6|     GXI|     GERRESHEIMER AG|Common stock|     EUR|   2504380|2022-01-03|2022-10-28 08:00:00|      85.3|    85.8|    85.2|    85.8|        1726|            10|\n",
      "|DE000A0WMPJ6|    AIXA|  AIXTRON SE NA O.N.|Common stock|     EUR|   2504428|2022-01-03|2022-10-28 08:00:00|     18.17|  18.175|   18.06|   18.06|       13598|            16|\n",
      "|DE000A0Z2ZZ5|    FNTN|  FREENET AG NA O.N.|Common stock|     EUR|   2504438|2022-01-03|2022-10-28 08:00:00|      23.4|    23.4|   23.39|   23.39|        4237|             2|\n",
      "|DE000A1DAHH0|     BNR| BRENNTAG SE NA O.N.|Common stock|     EUR|   2504453|2022-01-03|2022-10-28 08:00:00|     79.08|   79.34|   79.06|   79.28|       10106|            28|\n",
      "|DE000A1EWWW0|     ADS|   ADIDAS AG NA O.N.|Common stock|     EUR|   2504471|2022-01-03|2022-10-28 08:00:00|    254.35|   254.6|  254.05|  254.55|       13067|            61|\n",
      "|DE000A1H8BV3|    NOEJ|NORMA GROUP SE NA...|Common stock|     EUR|   2504474|2022-01-03|2022-10-28 08:00:00|      33.7|    33.7|    33.7|    33.7|          93|             1|\n",
      "|DE000A1J5RX9|     O2D|TELEFONICA DTLD H...|Common stock|     EUR|   2504492|2022-01-03|2022-10-28 08:00:00|     2.483|   2.484|   2.463|   2.463|       60387|            14|\n",
      "|DE000A1PHFF7|    BOSS|HUGO BOSS AG NA O.N.|Common stock|     EUR|   2504512|2022-01-03|2022-10-28 08:00:00|     53.86|   54.14|   53.86|   54.06|       12836|            30|\n",
      "|DE000A1X3XX4|     DIC|DIC ASSET AG NA O.N.|Common stock|     EUR|   2504532|2022-01-03|2022-10-28 08:00:00|     15.41|   15.41|   15.41|   15.41|        2093|             1|\n",
      "|DE000A12DM80|     G24|  SCOUT24 SE NA O.N.|Common stock|     EUR|   2504556|2022-01-03|2022-10-28 08:00:00|     61.74|   61.84|   61.74|   61.84|         422|             2|\n",
      "|DE000A13SX22|     HLE|HELLA GMBH+CO. KG...|Common stock|     EUR|   2504580|2022-01-03|2022-10-28 08:00:00|     62.14|   62.14|   61.78|   61.78|         819|             3|\n",
      "|DE000A161N30|     GLJ|   GRENKE AG NA O.N.|Common stock|     EUR|   2504606|2022-01-03|2022-10-28 08:00:00|     30.76|    30.8|   30.76|    30.8|         610|             2|\n",
      "|DE000BASF111|     BAS|     BASF SE NA O.N.|Common stock|     EUR|   2504663|2022-01-03|2022-10-28 08:00:00|      61.9|   62.05|    61.9|   62.02|      137362|            65|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+-------------------+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initial look of data\n",
    "\n",
    "df_initial.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b070ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting time column into time format\n",
    "\n",
    "from pyspark.sql.functions import date_format\n",
    "df_initial = df_initial.withColumn('Time', date_format('time', 'HH:mm:ss'))\n",
    "logging.info(\"Format of time updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5690410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+--------------------+------------+--------+----------+----------+--------+----------+--------+--------+--------+------------+--------------+\n",
      "|        ISIN|Mnemonic|        SecurityDesc|SecurityType|Currency|SecurityID|      Date|    Time|StartPrice|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+--------+----------+--------+--------+--------+------------+--------------+\n",
      "|AT0000A0E9W5|    SANT|         S+T AG O.N.|Common stock|     EUR|   2504159|2022-01-03|08:00:00|     14.76|   14.76|   14.75|   14.75|        4414|             2|\n",
      "|DE000A0DJ6J9|     S92|SMA SOLAR TECHNOL.AG|Common stock|     EUR|   2504287|2022-01-03|08:00:00|     37.64|   37.66|    37.6|   37.66|        1649|             3|\n",
      "|DE000A0D6554|    NDX1|      NORDEX SE O.N.|Common stock|     EUR|   2504290|2022-01-03|08:00:00|     13.99|   14.03|   13.94|   13.96|       23011|            36|\n",
      "|DE000A0D9PT0|     MTX|MTU AERO ENGINES ...|Common stock|     EUR|   2504297|2022-01-03|08:00:00|       180|  180.05|   179.5|   179.5|        2308|            22|\n",
      "|DE000A0HN5C6|    DWNI|DEUTSCHE WOHNEN S...|Common stock|     EUR|   2504314|2022-01-03|08:00:00|     37.28|   37.28|   37.28|   37.28|        2897|             1|\n",
      "+------------+--------+--------------------+------------+--------+----------+----------+--------+----------+--------+--------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  checking data after conversion\n",
    "\n",
    "df_initial.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d59e6bd",
   "metadata": {},
   "source": [
    "### Analysing using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55fec5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary view of our table\n",
    "\n",
    "df_initial.createOrReplaceTempView('df_initial_table')\n",
    "logging.info(\"Initial table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9f8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only necessary columns and save them in a dataframe. Two new columns also added to filter open price and close price for later stage\n",
    "\n",
    "df_initial_trans_1 = spark.sql(\"Select Date, Time, Mnemonic, StartPrice, row_number() over(partition by Date, Mnemonic order by Time) as for_start, row_number() over(partition by Date, Mnemonic order by Time Desc) as for_end, MaxPrice, MinPrice, EndPrice, TradedVolume, NumberOfTrades from df_initial_table\")\n",
    "logging.info(\"Unecessary columns filtered out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7113af16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+----------+---------+-------+--------+--------+--------+------------+--------------+\n",
      "|      Date|    Time|Mnemonic|StartPrice|for_start|for_end|MaxPrice|MinPrice|EndPrice|TradedVolume|NumberOfTrades|\n",
      "+----------+--------+--------+----------+---------+-------+--------+--------+--------+------------+--------------+\n",
      "|2022-01-03|16:36:00|    00XJ|    6.3345|        5|      1|  6.3345|  6.3345|  6.3345|           0|             2|\n",
      "|2022-01-03|15:03:00|    00XJ|     6.433|        4|      2|   6.433|   6.433|   6.433|          92|             1|\n",
      "|2022-01-03|14:44:00|    00XJ|       6.5|        3|      3|     6.5|     6.5|     6.5|         500|             1|\n",
      "|2022-01-03|12:12:00|    00XJ|    6.4345|        2|      4|  6.4345|  6.4345|  6.4345|           0|             1|\n",
      "|2022-01-03|08:04:00|    00XJ|    6.4435|        1|      5|  6.4435|  6.4435|  6.4435|           0|             1|\n",
      "+----------+--------+--------+----------+---------+-------+--------+--------+--------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data to see if new columns are populated correctly\n",
    "\n",
    "df_initial_trans_1.filter(df_initial_trans_1.Mnemonic == '00XJ').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "530fd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_1.createOrReplaceTempView('df_initial_trans_1_table')\n",
    "logging.info(\"Table created for 2nd query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b07b306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select high price, low price, total traded volumne and total number of trades\n",
    "\n",
    "df_initial_trans_2 = spark.sql(\"Select Date, Mnemonic, case when for_start = 1 then StartPrice else NULL end as open_price, case when for_end = 1 then EndPrice else NULL end as close_price, max(MaxPrice) over(partition by Date, Mnemonic) as High, min(MinPrice) over(partition by Date, Mnemonic) as Low, sum(TradedVolume) over(partition by Date, Mnemonic) as total_trades, sum(NumberOfTrades) over(partition by Date, Mnemonic) as NumberOfTrades from df_initial_trans_1_table\")\n",
    "\n",
    "logging.info(\"Query to select high price, low price, total traded volumne and total number of trades executed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81a0d95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+----+------+------------+--------------+\n",
      "|      Date|Mnemonic|open_price|close_price|High|   Low|total_trades|NumberOfTrades|\n",
      "+----------+--------+----------+-----------+----+------+------------+--------------+\n",
      "|2022-01-03|    00XJ|      null|     6.3345| 6.5|6.3345|       592.0|           6.0|\n",
      "|2022-01-03|    00XJ|      null|       null| 6.5|6.3345|       592.0|           6.0|\n",
      "|2022-01-03|    00XJ|      null|       null| 6.5|6.3345|       592.0|           6.0|\n",
      "|2022-01-03|    00XJ|      null|       null| 6.5|6.3345|       592.0|           6.0|\n",
      "|2022-01-03|    00XJ|    6.4435|       null| 6.5|6.3345|       592.0|           6.0|\n",
      "+----------+--------+----------+-----------+----+------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data to see if new columns are populated correctly\n",
    "\n",
    "df_initial_trans_2.filter(df_initial_trans_2.Mnemonic == '00XJ').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85c14aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_2.createOrReplaceTempView('df_initial_trans_2_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4997a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to remove the null rows from the data\n",
    "\n",
    "df_initial_trans_3 = spark.sql(\"Select Date, Mnemonic, round(max(open_price),2) as open_price, round(max(close_price),2) as close_price, round(max(High),2) as High, round(max(Low),2) as Low, max(total_trades) as total_trades, max(NumberOfTrades) as NumberOfTrades from df_initial_trans_2_table group by Date, Mnemonic\")\n",
    "logging.info(\"Table created for 3rd query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff98afb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+\n",
      "|      Date|Mnemonic|open_price|close_price| High|  Low|total_trades|NumberOfTrades|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+\n",
      "|2022-01-03|    SANT|     14.76|      15.62| 15.7|14.75|    370776.0|         717.0|\n",
      "|2022-01-04|    SANT|     15.66|      14.96|15.68|14.96|    589461.0|         973.0|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query to check if data is getting populated correctly\n",
    "\n",
    "df_initial_trans_3.filter(df_initial_trans_3.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82ba10ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_3.createOrReplaceTempView('df_initial_trans_3_table')\n",
    "logging.info(\"Table created for 4th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9720a27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_4 = spark.sql(\"Select *, Lag(close_price,1) over(partition by Mnemonic order by Date) as previous_day from df_initial_trans_3_table\")\n",
    "logging.info(\"Query to create a lag column which will be used for calculating the closing price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd34fcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+\n",
      "|      Date|Mnemonic|open_price|close_price| High|  Low|total_trades|NumberOfTrades|previous_day|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+\n",
      "|2022-01-03|    SANT|     14.76|      15.62| 15.7|14.75|    370776.0|         717.0|        null|\n",
      "|2022-01-04|    SANT|     15.66|      14.96|15.68|14.96|    589461.0|         973.0|       15.62|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Query to check if data is getting populated correctly\n",
    "\n",
    "df_initial_trans_4.filter(df_initial_trans_4.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e0816d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_4.createOrReplaceTempView('df_initial_trans_4_table')\n",
    "logging.info(\"Table created for 5th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "770ee01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_5 = spark.sql(\"Select *, case when previous_day is NULL then 0 else round((close_price - previous_day),2) end as close_price_diff from df_initial_trans_4_table\")\n",
    "logging.info(\"Query to calculate price difference is calculated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dcd74969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+----------------+\n",
      "|      Date|Mnemonic|open_price|close_price| High|  Low|total_trades|NumberOfTrades|previous_day|close_price_diff|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+----------------+\n",
      "|2022-01-03|    SANT|     14.76|      15.62| 15.7|14.75|    370776.0|         717.0|        null|             0.0|\n",
      "|2022-01-04|    SANT|     15.66|      14.96|15.68|14.96|    589461.0|         973.0|       15.62|           -0.66|\n",
      "+----------+--------+----------+-----------+-----+-----+------------+--------------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Query to check if data is getting populated correctly\n",
    "\n",
    "df_initial_trans_5.filter(df_initial_trans_5.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "58d33fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_5.createOrReplaceTempView('df_initial_trans_5_table')\n",
    "logging.info(\"Table created for 6th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afc691d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_6 = spark.sql(\"Select weekofyear(Date) as Week_of_Year, Date, Mnemonic, open_price, close_price, High, Low, close_price_diff, NumberOfTrades, total_trades from df_initial_trans_5_table\")\n",
    "logging.info(\"Adding week of the year in the table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8126b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+-----------+-----+-----+----------------+--------------+------------+\n",
      "|Week_of_Year|      Date|Mnemonic|open_price|close_price| High|  Low|close_price_diff|NumberOfTrades|total_trades|\n",
      "+------------+----------+--------+----------+-----------+-----+-----+----------------+--------------+------------+\n",
      "|           1|2022-01-03|    SANT|     14.76|      15.62| 15.7|14.75|             0.0|         717.0|    370776.0|\n",
      "|           1|2022-01-04|    SANT|     15.66|      14.96|15.68|14.96|           -0.66|         973.0|    589461.0|\n",
      "+------------+----------+--------+----------+-----------+-----+-----+----------------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_initial_trans_6.filter(df_initial_trans_6.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69864f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_6.toPandas().to_csv(\"Converted\\Weekly_Record\\Overall_Days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db2729f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_6.createOrReplaceTempView('df_initial_trans_6_table')\n",
    "logging.info(\"Table created for 7th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56e8ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_week = spark.sql(\"Select Week_of_Year, Mnemonic, round(avg(open_price),2) as op_week, round(avg(close_price),2) as cp_week, round(avg(High),2) as high_week, round(avg(Low),2) as low_week, round(sum(NumberOfTrades),2) as NumberOfTrades, round(sum(total_trades),2) as total_trades from df_initial_trans_6_table group by Week_of_Year, Mnemonic\")\n",
    "logging.info(\"Table created for week wise data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "932d8fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+-------+---------+--------+--------------+------------+\n",
      "|Week_of_Year|Mnemonic|op_week|cp_week|high_week|low_week|NumberOfTrades|total_trades|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+\n",
      "|           1|    SANT|  15.21|  15.29|    15.69|   14.86|        1690.0|    960237.0|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_initial_trans_week.filter(df_initial_trans_week.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e8ed2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_week.createOrReplaceTempView('df_initial_trans_week_table')\n",
    "logging.info(\"Table created for 8th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49078d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_week_2 = spark.sql(\"Select *, LAG(cp_week, 1) over(partition by Mnemonic order by Week_of_Year) as close_price_dff from df_initial_trans_week_table\")\n",
    "logging.info(\"Table created for weekly price difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a651d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "|Week_of_Year|Mnemonic|op_week|cp_week|high_week|low_week|NumberOfTrades|total_trades|close_price_dff|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "|           1|    SANT|  15.21|  15.29|    15.69|   14.86|        1690.0|    960237.0|           null|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_initial_trans_week_2.filter(df_initial_trans_week_2.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e7270de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temp table after filteration\n",
    "\n",
    "df_initial_trans_week_2.createOrReplaceTempView('df_initial_trans_week_2_table')\n",
    "logging.info(\"Table created for 9th query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de75b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_week_3 = spark.sql(\"Select Week_of_Year, Mnemonic, op_week, cp_week, high_week, low_week, NumberOfTrades, total_trades, Case when close_price_dff is NULL then 0 else round((close_price_dff - cp_week),2) end as close_week_diff from df_initial_trans_week_2_table\")\n",
    "logging.info(\"Creating weekly difference column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b4258179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "|Week_of_Year|Mnemonic|op_week|cp_week|high_week|low_week|NumberOfTrades|total_trades|close_week_diff|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "|           1|    SANT|  15.21|  15.29|    15.69|   14.86|        1690.0|    960237.0|            0.0|\n",
      "+------------+--------+-------+-------+---------+--------+--------------+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_initial_trans_week_3.filter(df_initial_trans_week_3.Mnemonic == 'SANT').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "080ead2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_initial_trans_week_3.toPandas().to_csv(\"Converted\\Weekly_Record\\Overall_Weeks.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c655a2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_kernel",
   "language": "python",
   "name": "spark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
